Can radioactive decay rates be confirmed by looking at supernova?

Radiometric dating relies on accurate knowledge of the half-lives of radioactive elements. Skeptics have argued that since many elements' half-lives are too long for anyone to have observed them, we can't know what they were in the distant past.
There are various counterarguments based on terrestrial evidence, like agreement between elements in the same sample, and corroboration from varves, tree rings, and ice layers.
Another counterargument I've seen is by looking at supernova, we get a window "back in time" to when their light began its long journey to us, and that much of this light was generated by radioactive decay, confirming that decay rates were the same then as now.
I don't quite follow this explanation, and would like some help. Distant starlight doesn't give us a rock sample in which to analyze proportions of parent and daughter elements. Does it still somehow give evidence of specific radioactive decay rates? If so, how?
I know next to nothing about astronomy, so please explain this to me like I'm a child.