Early universe's first starlight (indirectly) detected for the first time, could it have been detectable with 1970's technology?

The recent  paper in Nature An absorption profile centred at 78 megahertz in the sky-averaged spectrum seems (to me at least) to be an incredibly simple and elegant experiment with far-reaching implications.
While the system has been implemented very carefully, it is essentially a spectrally resolved radiometer at radio frequencies below 200 MHz, measuring the sky-averaged power spectrum. And while many of radiometers in space and on Earth measuring the Cosmic Microwave Background are fairly broad band, measure in the 10's to 100's of GHz range, and intended for very precise absolute intensity measurements of a smooth Plank distribution, the system described here measures between only 50 and 200 MHz, and with a spectral resolution of only 6.1 kHz!
The small set of carefully designed and optimized, antenna(s) and receiver(s) are simple and straightforward; a broadband dipole-type antenna above a large ground plane in a radio-quiet zone, some low noise electronics and what non-radio astronomers might call a very large bandwidth Software-defined Radio or SDRs.
And yet this small, simple receiver, sitting quietly on the ground in Australia's Outback has produced observations are amazing! They've detected direct evidence of the first stars producing the first starlight in our early universe only about 200 million years after the Big Bang!

Abstract:
After stars formed in the early Universe, their ultraviolet light is expected, eventually, to have penetrated the primordial hydrogen gas and altered the excitation state of its 21-centimetre hyperfine line. This alteration would cause the gas to absorb photons from the cosmic microwave background, producing a spectral distortion that should be observable today at radio frequencies of less than 200 megahertz1. Here we report the detection of a flattened absorption profile in the sky-averaged radio spectrum, which is centred at a frequency of 78 megahertz and has a best-fitting full-width at half-maximum of 19 megahertz and an amplitude of 0.5 kelvin. The profile is largely consistent with expectations for the 21-centimetre signal induced by early stars; however, the best-fitting amplitude of the profile is more than a factor of two greater than the largest predictions2. This discrepancy suggests that either the primordial gas was much colder than expected or the background radiation temperature was hotter than expected. Astrophysical phenomena (such as radiation from stars and stellar remnants) are unlikely to account for this discrepancy; of the proposed extensions to the standard model of cosmology and particle physics, only cooling of the gas as a result of interactions between dark matter and baryons seems to explain the observed amplitude3. The low-frequency edge of the observed profile indicates that stars existed and had produced a background of Lyman-α photons by 180 million years after the Big Bang. The high-frequency edge indicates that the gas was heated to above the radiation temperature less than 100 million years later.


I don't see anything about this data that strikes me as unmeasurable with even say 1970's technology if done carefully. The signal is huge, a distinct, broad (about 19 MHz centered at about 87 MHz), several percent (almost 0.6 K) deep dip in an otherwise Rayleigh-Jeans regime blackbody distribution. In 20:20 hindsight, it seems to me this could have been detected a long time ago.
Question: Am I missing something, or is this discovery remarkable not only for it's far reaching implication, but for its simplicity, elegance, and the fact that it could have been discovered quite a long time ago with '60's or '70's technology?

above: Figure 1 from the paper.
below: Figures 2 and then 1 from the paper's Extended data.