How do you fit a model SED to an observed galaxy spectrum with photometric data?

I have a spectrum of a galaxy from HST data and SDSS data (resulting in a nearly continuous SED from the FUV~1000 Angstroms through infrared). I have created a model SED with the galaxy simulation software STARBURST99 (for the correct star formation, age and type of evolution for the real galaxy) to extend from ~90 Angstroms through the infrared. I then reddened my model spectrum with the Cardelli et al. (1989) extinction law. I also have various photometric multi-band data from different surveys. I understand that I need to use this data to adjust my model spectrum so that it is a good fit of the observational SED and matches observations at each photometric point. But I'm confused on how I would go about this normalization/calibration process of my model spectrum. 
Is it as simple as linear offsets in flux at these various different points? I doubt that, however, because that would then seem difficult to keep my model spectrum smooth. I'm unsure if I really understand the process of proper SED fitting in the first place... Any help with this concept and how I could use photometric multi-band data to increase the goodness of fit of my model is greatly appreciated!