Should I drop source shot noise from the SNR calculation when all I care about is detection (not photometric precision)?

All of the calculations for signal-to-noise ratio (SNR) of sources in astronomical images include various noise terms of relevance: read noise, quantization noise, and shot noise from each of the background, dark noise, and source. If I'm understanding correctly, though, this SNR calculation provides the SNR of the measurement of the photometry/brightness of the source; that is, the precision at which you measured its brightness.
If, however, all I care about is whether or not a purported source is likely to be real and not caused by noise, then to me it seems that the SNR calculation should compare the signal with only the noise that would be there if the purported source wasn't. That is, only the detector and background noise terms, not the source shot noise. This provides a measure of the signal strength against the noise terms that could contribute to a spurious signal. The shot noise of a signal itself does not factor into whether that signal should be regarded as spurious.
Is this correct, to regard SNR calculations for photometric measurements and raw detections as separate? And to not include the source shot noise in the latter? If so, any good sources for that in case I have to convince someone else of this?