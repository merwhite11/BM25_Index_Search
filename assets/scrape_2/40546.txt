Why is the flux density and amplitude different for galaxies than stars?

This might be a simple question, but I'm having a hard time answering it myself - or at least answer it correctly, I think.
I'm sitting with different spectrals; many for stars and one for a galaxy. The one for the galaxy has a flux density of 1e-13, while the stars range from 1e-7 to 1e-12.
My question is: why is galaxy's flux density lower than even the lowest stars? The galaxy also have very characteristic emission peaks. How is that reflected in the spectrum of the stars, or is that another phenomena?
To clarify, I am talking about a plot like this. I had to scale the stars spectrum down by a lot in order to match the galaxy's spectrum. Why is that?
Also why can't the stars produce the same spiky amplitudes as the galaxy? Is it because of the galaxy emission lines from billions of stars and vast amounts of dust and gas?