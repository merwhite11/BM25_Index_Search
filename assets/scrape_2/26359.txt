Does the focus of terrestial telecopes need adjustment for the distance of astronomical objects?

My rudimentary experience of photography and telescopes suggests that with commonly affordable optics any object beyond the range of hundreds to thousands of feet will be in adequate focus at the proper "infinity" position. For example the advice here for overcoming difficulties in seeing the focus of a star on a DLSR screen to instead focus on a brighter and distant city light: https://photo.stackexchange.com/a/23973. 
However an interlocutor recently offered this advice (collated from a series of comments):
Apparently you haven't actually used a camera or a telescope. The focus adjustment is there for a reason. Any astronomer will tell you focus changes are needed between stars, stars farther away,  planets, and the Moon. ... If focused on the Moon, you need to refocus slightly when looking at stars. When looking at Jupiter you need to refocus if the target is the Moon. Close stars vs far away stars, the same thing, a need to refocus. ...  To the tune of: Jupiter and it's moons, which are close to Jupiter , need focus adjustment between them. Either Jupiter or it's moons will be in focus. Ask any astronomer. That's a difference and it's well past the supposed "infinity focus" point.
I wouldn't be wholly surprised if retargeting a telescope could require a focus tweak to correct for slight changes of the mirror distances (etc), or from thermal shrinkage across a cooling evening, but such effects would probably be invariant across objects, whereas as described in that quote targetting back on the original object would presumably require the focus to be restored to the original setting. 
Is there a distance-dependent component for focus in astronomical observation, and if so what sort of equipment does it affect in practice?