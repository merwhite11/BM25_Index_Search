How much does foreground clumping affect the estimated sizes of high-redshift objects?

I've always converted angular to linear size by using the angular-diameter distance without really thinking about it. But in light of recent stories about high-$z$ galaxies observed by JWST being unexpectedly small, I thought about it, and now wonder if it makes any sense.
The problem is that distant objects are seen in directions where they aren't obscured by closer objects. If you consider a tube of spacetime containing the geodesics of light emitted from opposite ends of the high-$z$ object and ending up at a telescope, the matter density in the tube should be lower than average, so there should be less positive lensing. If the matter density was zero, or merely low enough for $Λ$ to dominate, the geodesics traced backward from the telescope would never reconverge. At higher densities, it seems like the turnaround from divergence to convergence could be at significantly higher $z$ than the average over the whole sky. And that's what the JWST seems to be seeing (I've heard it alleged).
Another way of looking at it is that foreground clumps of matter magnify and duplicate what's directly behind them, and the total area of the sky is fixed ($4π\text{ sr}$), so other distant objects must appear to shrink, relative to their angular size if the foreground matter were homogeneous.
My questions are: what has been published about this effect, and how large actually is it?