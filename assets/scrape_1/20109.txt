How to convert theoretical template spectrum from luminosity density to flux density units?

I'm working with galaxy spectral templates (e.g., Bruzual & Charlot 2003) which seem to always come with y-axis units of $L_{\odot}$/A and x-axis units of Angstroms. Thus the y-axis is a luminosity density instead of a flux density. In contrast, observationally, we tend to always work with spectra that have y-axis units of flux density ($F_{\lambda}$ in erg/s/cm$^2$/A or $F_{\nu}$ in erg/s/cm$^2$/Hz). Similarly, spectral energy distributions (SEDs) from photometry tend to have $\lambda F_{\lambda}$ or $\nu F_{\nu}$ such that the y-axis is flux, not flux density. 
How do I convert a theoretical template spectrum from units of luminosity density ($L_{\odot}$/A) to flux density (erg/s/cm$^2$/A)? 
For context, I want to fit spectral templates to an observed SED. The observed SED is for an object at a redshift $z$, so I think I can either convert the templates to flux density units, or I can convert my observed SED to luminosity density units. I feel like working in flux density units is more natural -- plus I'm not sure if multiplying the observed SED y-axis values by $4\pi D^2$ (D is distance of object) and x-axis (wavelengths) by $1/(1+z)$ would be sufficient (e.g., normalization concerns).